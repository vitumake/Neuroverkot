{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Get pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "path = r\"C:\\Users\\markus\\.keras\\datasets\\glove.6B\\glove.6B.100d.txt\"\n",
    "\n",
    "def load_glove_embeddings(file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(file_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "    return embeddings_index\n",
    "\n",
    "# Load GloVe embeddings\n",
    "glove_embeddings = load_glove_embeddings(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: king - 0.8551837205886841\n",
      "2: queen - 0.7834413647651672\n",
      "3: monarch - 0.6933802366256714\n",
      "4: throne - 0.6833109855651855\n",
      "5: daughter - 0.6809083223342896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Get the word vectors for \"man\", \"woman\", and \"king\"\n",
    "vec_man = glove_embeddings['man']\n",
    "vec_woman = glove_embeddings['woman']\n",
    "vec_king = glove_embeddings['king']\n",
    "\n",
    "# Calculate the vector for the expression: vec(\"woman\") - vec(\"man\") + vec(\"king\")\n",
    "result_vector = vec_woman - vec_man + vec_king\n",
    "\n",
    "# Function to find the most similar word\n",
    "def find_most_similar(vector, embeddings, top_n=1):\n",
    "    similarities = {}\n",
    "    for word, embedding in embeddings.items():\n",
    "        similarity = cosine_similarity([vector], [embedding])[0][0]\n",
    "        similarities[word] = similarity\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    return sorted_similarities[:top_n]\n",
    "\n",
    "# Find the nearest vector(s) to the result_vector\n",
    "nearest_words = find_most_similar(result_vector, glove_embeddings, top_n=5)\n",
    "\n",
    "for i, (word, similarity) in enumerate(nearest_words, 1):\n",
    "    print(f\"{i}: {word} - {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "The result of the expression `vec(\"woman\") - vec(\"man\") + vec(\"king\")` is expected to be close to the vector for the word \"queen\". This is because the vector arithmetic captures the relationship between these words in the embedding space, where the difference between \"man\" and \"woman\" is similar to the difference between \"king\" and \"queen\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
